{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install -qU guardrails-ai lark openai langchain_community langchain_experimental langchain-upstage sentence-transformers langchainhub langchain-chroma langchain matplotlib python-dotenv tavily-python ragas faiss-cpu tokenizers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv\n",
    "# set UPSTAGE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "from langchain_upstage import ChatUpstage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.schema import AIMessage, HumanMessage\n",
    "\n",
    "\n",
    "llm = ChatUpstage()\n",
    "\n",
    "from tavily import TavilyClient\n",
    "import os\n",
    "\n",
    "# tavily = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More general chat\n",
    "chat_with_history_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{message}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "rag_with_history_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question considering the history of the conversation. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "---\n",
    "CONTEXT:\n",
    "{context}\n",
    "        \"\"\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{message}\"),\n",
    "    ]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = rag_with_history_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_upstage import UpstageEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "\n",
    "from langchain_upstage import UpstageGroundednessCheck\n",
    "groundedness_check = UpstageGroundednessCheck()\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    persist_directory = \"C:/Users/JunMung/Desktop/mystudy/kaist/upstage/cookbook/data/taeho/chroma_db203\",\n",
    "    embedding_function=UpstageEmbeddings(model=\"solar-embedding-1-large\"),\n",
    ")\n",
    "\n",
    "index_count = vectorstore._collection.count()\n",
    "print(f\"Vector store index count: {index_count}\")\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\":3})\n",
    "\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "\n",
    "\n",
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"subject\",\n",
    "        description=\"The subject of the email\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"from\",\n",
    "        description=\"who sent the email\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"to\",\n",
    "        description=\"who received the email\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"date\", description=\"\\\"YYYY-MM-DDTHH:MM:SS\\\"\", type=\"string\"\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"uid\", description=\"email unique id\", type=\"string\"\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"cc\", description=\"carbon copy\", type=\"string\"\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "document_content_description = \"The content of the email\"\n",
    "\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm,\n",
    "    vectorstore,\n",
    "    document_content_description,\n",
    "    metadata_field_info,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_retrieved_docs(query):\n",
    "    results_docs = retriever.invoke(query)\n",
    "\n",
    "    return results_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    results_docs = get_retrieved_docs(message)\n",
    "    history_langchain_format = []\n",
    "    for human, ai in history:\n",
    "        history_langchain_format.append(HumanMessage(content=human))\n",
    "        history_langchain_format.append(AIMessage(content=ai))\n",
    "\n",
    "    for _ in range(5):\n",
    "        response = chain.invoke({\n",
    "            \"message\": message, \n",
    "            \"context\": results_docs,\n",
    "            \"history\": history_langchain_format\n",
    "        })\n",
    "        gc_result = groundedness_check.invoke({\"context\":results_docs,\"answer\":response})\n",
    "        print(results_docs)\n",
    "        print(response)\n",
    "        print(\"GC check result: \", gc_result)\n",
    "        if gc_result.lower().startswith(\"grounded\"):\n",
    "            print(\"✅ Groundedness check passed\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"❌ Groundedness check failed\")\n",
    "            continue\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.ChatInterface(\n",
    "        chat,\n",
    "        examples=[\n",
    "            \"How to eat healthy?\",\n",
    "            \"Best Places in Korea\",\n",
    "            \"How to make a chatbot?\",\n",
    "        ],\n",
    "        title=\"Solar Chatbot\",\n",
    "        description=\"Upstage Solar Chatbot\",\n",
    "    )\n",
    "    chatbot.chatbot.height = 300\n",
    "    outputs=[gr.Textbox(label=\"Retrieved Email List\")],\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
